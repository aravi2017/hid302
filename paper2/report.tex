\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Hadoop and MongoDB in support of Big Data Applications and Analytics}


\author{Sushant Athaley}
\affiliation{%
  \institution{Indiana University}
}
\email{sathaley@iu.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
Big data processing is beyond capability of traditional tool. It requires specialized tools like Hadoop and MongoDB. We will explore Haddop and MongoDB technically as a tool and how they provide support/help in big data analysis.
TBD
\end{abstract}

\keywords{i523, hid302, big data, Hadoop, MongoDB}


\maketitle


\section{Introduction}

Describe about big data, hadoop and mongodb. Describe what this paper will do.
Papers organization.

\section{Big Data}
Big Data is defined in lot many different ways but one of the interesting ways it has been defined is in terms of three V's which are Volume, Velocity, and Variety. Big data is generated in great \emph{volume} typically in the gigabyte or more which makes data processing difficult. Data \emph{velocity} has been increased due to the real-time data streaming from various applications like social media or different type of sensors recording data continuously. Big data comes in \emph{variety} of format like structured or unstructured data. Data varies in various format like text, pictures, audio, videos, 3D, social media and so on. These big data characteristics pose challenges in terms of overall data lifecycle management. Some of the examples of big data usage are the recommendation service, predictive analytics, data analytics, pattern identification, and machine learning. Traditional systems are good for small or medium data processing but unable to provide support for the big data. Big data need specialized technologies and tools to handle its characteristics. The technologies which can solve big data problem should have capabilities like distributed computing system, massively parallel processing, NoSQL, and analytical database \cite[Ch.\ 1, p. 4]{AchariShiva2015HE}. Can Hadoop or MongoDB be those technologies who can provide that support?  

\section{Hadoop}
Apache foundation describes Hadoop as ``The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures'' \cite{www-hadoop}. In other words, Hadoop provides a framework to store data in the distributed manner and provides the capability to run data analysis in the distributed way.

``Currently Hadoop project includes following modules:
\begin{itemize}
\item {\bf Hadoop Common}: The common utilities that support the other Hadoop modules.
\item {\bf Hadoop Distributed File System (HDFS)}: A distributed file system that provides high-throughput access to application data.
\item {\bf Hadoop YARN}: A framework for job scheduling and cluster resource management.
\item {\bf Hadoop MapReduce}: A YARN-based system for parallel processing of large data sets'' \cite{www-hadoop}.
\end{itemize}

\subsection{Hadoop Common}
Hadoop Common are the collection of the utilities to support the Hadoop modules. This is the core package which provide essential and basic service of the framework.

\subsection{Hadoop Distributed File System (HDFS)}
Hadoop Distributed File System (HDFS) is the default distributed file system provided by the Hadoop. HDFS serves as storage mechanism in the Hadoop framework. HDFC specifically designed to process large data set and run on low cost hardware. It is high fault tolerant which contains mechanism for quick fault detection and auto recovery. HDFS is designed to port across heterogeneous hardware and software platform. It does data computation on same node instead of moving data to the server which is faster as well as avoid network congestion. It provides scalability by adding or removing nodes in the HDFS cluster and can support hundreds of nodes in single cluster \cite{www-hdfs-arch}. Figure \ref{f:hdfs-arch} shows HDFS architecture.
\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/hdfsArch.PNG}
  \caption{HDFS Architecture \cite{www-hdfs-arch}}\label{f:hdfs-arch}
\end{figure}

HDFS is based on master/slave architecture where NameNode is the master server and DataNodes are the slave nodes. There can be only one NameNode server which manages file system name space and all read write requests. NameNode doesn't store any data but contains all the meta-data about files and DataNodes. DataNode contains actual data and they can be multiple in numbers usually one per node. DataNodes are responsible for the create, delete, replicate of the datablocks on the node as per the instruction by the NameNode. DataNode also sends block-report to NameNode which has list of all blocks on the DataNode. DataNode sends heartbeat message to NameNode which helps in identifying the failure nodes. If heartbeat is not received by NameNode in specified interval then that DataNode is marked as dead and NameNode usage different DataNode. Figure \ref{f:hdfs-read} and \ref{f:hdfs-write} depicts read and write in HDFS respectively.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/hdfsRead.PNG}
  \caption{HDFS Read \cite[Ch.\ 3, p. 38]{AchariShiva2015HE}}\label{f:hdfs-read}
\end{figure}

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/hdfsWrite.PNG}
  \caption{HDFS Write \cite[Ch.\ 3, p. 39]{AchariShiva2015HE}}\label{f:hdfs-write}
\end{figure}


\subsection{Hadoop YARN}
Hadoop YARN (Yet Another Resource Negotiator) provides cluster resource management which helps in running multiple distributed application in Hadoop. YARN consists of 3 components \emph{ResourceManager (RM), NodeManager (NM)} and \emph{ApplicationMaster (AM)}. ResourceManager is the master process which manages resources across the nodes. NodeManager is responsible for the container and provide resource usage to the ResourceManager. ApplicationMaster is responsible for getting resources from ResourceManager and work with NodeManager to execute the task \cite{www-apache-yarn}. YARN makes it possible to run different application on Hadoop platform which makes it scalable and integrable \cite[Ch.\ 3, p. 65]{AchariShiva2015HE}.
Figure \ref{f:yarn-arch} shows YARN architecture.
\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/yarnArch.PNG}
  \caption{YARN Architecture \cite{www-apache-yarn}}\label{f:yarn-arch}
\end{figure}

\subsection{Hadoop MapReduce}
Hadoop MapReduce is a framework which provides capability to process vast amount of data in distributed manner. Processing is done in parallel on various nodes utilizing local machine processor and memory which results in high computation power. Framework provides fault tolerance along with supporting large clusters usually thousands of nodes. Typical framework processing is to split input data into independent chunks and then processed by \emph{map} tasks in parallel. Sort the output of the map task and then provide that as input to \emph{reduce} task for aggregate processing. Two important class in this framework are org.apache.hadoop.mapreduce.Mapper and org.apache.hadoop.mapreduce.Reducer. They respectively provides map and reduce method to process the data. 

Figure \ref{f:mapreduceex} shows MapReduce process using wordcount example. Each line in input file is passed to individual mapper class. Mapper class parses the line and sets count for the word. Sort and shuffle consolidates the data and sends it to reducer. Reducer performs the final word count and provides the output. 
\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/mapReduceEx.PNG}
  \caption{MapReduce Example \cite[Ch.\ 3, p. 48]{AchariShiva2015HE}}\label{f:mapreduceex}
\end{figure}

\subsection{Big Data Support}
Big data problem solution requires tools which can process huge amount of data with high computation power. Hadoop provides this capability by processing data in distributed environment in big clusters using MapReduce and also provides distributed storage system as HDFS. HDFS can be configured in a cluster of hundreds of nodes and can typically store file in size of gigabytes or terabytes \cite{www-hdfs-arch}. Using CPU and memory of local nodes delivers great computation power. Hadoop also provides high tolerance to the faults and scalability by adding nodes and integration with various technologies. Being open source and configurable on commodity hardware, Hadoop is cost effective and can be used by small industries as well for their big data solution. Hadoops capability to process large scale data in parallel within distributed environment makes it one of the best tool for Big Data processing. Hadoop is suported by various sub projects which together forms Hadoop Ecosystem. Different applications can be integrated in Hadoop depending on the big data problem need to be solved.  
Figure \ref{f:hadoopeco} illustrates Hadoop ecosystem by various layers.
\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/hadoopEcosys.PNG}
  \caption{Hadoop Ecosystem \cite[Ch.\ 2, p. 26]{AchariShiva2015HE}}\label{f:hadoopeco}
\end{figure}

Yahoo has one of the biggest Hadoop cluster. It has more than 100,000 CPUs in 40,000 computers running Hadoop. Their biggest cluster is of 4500 nodes. Yahoo is using Hadoop in research of Ad systems and web search and also used to do scaling tests to support development of Apache Hadoop on larger clusters \cite{www-apache-poweredby}.

Facebook uses Apache Hadoop  to store copies of internal log and dimension data sources and use it as a source for reporting/analytics and machine learning. They have 2 major Hadoop cluster, a 1100-machine cluster with 8800 cores and about 12 PB raw storage and a 300-machine cluster with 2400 cores and about 3 PB raw storage. Each node has 8 cores and 12 TB of storage \cite{www-apache-poweredby}. 

Ebay has 532 nodes Hadoop cluster. They are heavy user of Mapreduce, Apache Pig, Apachae Hive and Apache Hbase. They are using Hadoop for search optimization and research \cite{www-apache-poweredby}. 

\section{MongoDB}


\section{Tables}

In case you need to create tables, you can do this with online tools
(if you do not mind sharing your data) such as
\url{https://www.tablesgenerator.com/} or other such tools (please
google for them). They even allow you to manage tables as CSV.

or generate them by hand while using the provided template in Table\ref{t:mytable}. Not ethat
the caption is before the tabular environment.

\begin{table}[htb]
\centering
\caption{My caption}
\label{t:mytabble}
\begin{tabular}{lll}
1 & 2 & 3 \\
\hline
4 & 5 & 6 \\
7 & 8 & 9
\end{tabular}
\end{table}

\section{Long example}

If you like to see a more elaborate example, please look at
report-long.tex. 

\section{Conclusion}

Put here an conclusion. Conlcusions and abstracts must not have any
citations in the section.


\begin{acks}

  The authors would like to thank Dr. Gregor von Laszewski for his
  support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\appendix

We include an appendix with common issues that we see when students
submit papers. One particular important issue is not to use the
underscore in bibtex labels. Sharelatex allows this, but the
proceedings script we have does not allow this.

When you submit the paper you need to address each of the items in the
issues.tex file and verify that you have done them. Please do this
only at the end once you have finished writing the paper. To d this
cange TODO with DONE. However if you check something on with DONE, but
we find you actually have not executed it correcty, you will receive
point deductions. Thus it is important to do this correctly and not
just 5 minutes before the deadline. It is better to do a late
submission than doing the check in haste. 

\input{issues}

\end{document}
